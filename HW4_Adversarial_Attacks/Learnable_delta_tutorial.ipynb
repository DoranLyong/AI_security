{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim \n",
    "from torchvision import datasets, transforms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = \"data/lenet_mnist_model.pth\"\n",
    "use_cuda = True\n",
    "device = torch.device(\"cuda\" if (use_cuda and torch.cuda.is_available()) else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST Test dataset and dataloader declaration\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./dataset', train=False, download=True, transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            ])),\n",
    "        batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
       "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the network\n",
    "model = Net().to(device)\n",
    "\n",
    "# Load the pretrained model\n",
    "model.load_state_dict(torch.load(pretrained_model, map_location='cpu'))\n",
    "\n",
    "# Set the model in evaluation mode. In this case this is for the Dropout layers\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple Module to normalize an image\n",
    "class Normalize(nn.Module):\n",
    "    def __init__(self, mean, std):\n",
    "        super(Normalize, self).__init__()\n",
    "        self.mean = torch.Tensor(mean)\n",
    "        self.std = torch.Tensor(std)\n",
    "    def forward(self, x):\n",
    "        return (x - self.mean.type_as(x)[None,:,None,None]) / self.std.type_as(x)[None,:,None,None]\n",
    "\n",
    "# values are standard normalization for ImageNet images, \n",
    "# from https://github.com/pytorch/examples/blob/master/imagenet/main.py\n",
    "norm = Normalize(mean=[0.485], std=[0.229])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, target = next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f6989f43940>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image = torch.squeeze(data)\n",
    "plt.imshow(image, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, target = data.to(device), target.to(device )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -5.722029527532868e-06\n",
      "5 -4.124556289752945e-05\n",
      "10 -0.00025519452174194157\n",
      "15 -0.0015887507470324636\n",
      "20 -0.013099328614771366\n",
      "25 -0.2094910591840744\n",
      "True class probability: 0.19402503967285156\n"
     ]
    }
   ],
   "source": [
    "epsilon = 0.3\n",
    "\n",
    "delta = torch.zeros_like(data, requires_grad=True)\n",
    "opt = optim.Adam([delta], lr=5e-3)\n",
    "\n",
    "for t in range(30):\n",
    "    pred = model(norm(data + delta))\n",
    "    loss = -nn.CrossEntropyLoss()(pred, target)\n",
    "    if t % 5 == 0:\n",
    "        print(t, loss.item())\n",
    "    \n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    delta.data.clamp_(-epsilon, epsilon)\n",
    "#    print(delta[0][0][0][0])\n",
    "    \n",
    "print(\"True class probability:\", nn.Softmax(dim=1)(pred)[0,0].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2]], device='cuda:0')"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.max(1, keepdim=True)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_env] *",
   "language": "python",
   "name": "conda-env-pytorch_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
