{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ```ResNet``` significantly changed the view of how to parametrize the functions in deep networks.\n",
    "    * ```DenseNet``` is to some extent the logical extension of this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ```Dense_Blocks```\n",
    "* ```DenseNet``` uses the modified batch normalization, activation, and convolution architecture of ResNet (see the exercise in the previous section).\n",
    "\n",
    " Implement this architecture in the ```conv_block``` function.\n",
    " \n",
    "* A ```Dense_Blocks``` consists of multiple ```conv_block``` units, each using the same number of output channels. \n",
    "\n",
    "* In the forward computation, however, we concatenate the input and output of each block on the channel dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(input_channels, num_channels):\n",
    "    layers = []\n",
    "    layers.append(nn.BatchNorm2d(input_channels))\n",
    "    layers.append(nn.ReLU())\n",
    "    layers.append(nn.Conv2d(input_channels, num_channels, kernel_size=3, padding=1))\n",
    "    blk = nn.Sequential(*layers)\n",
    "    return blk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseBlock(nn.Module):\n",
    "    def __init__(self, num_convs, input_channels, num_channels, **kwargs):\n",
    "        super(DenseBlock, self).__init__(**kwargs)\n",
    "        layer = []\n",
    "        for i in range(num_convs):\n",
    "            layer.append(conv_block((num_channels * i + input_channels), num_channels))\n",
    "        self.net = nn.Sequential(*layer)\n",
    "    def forward(self, X):\n",
    "        for blk in self.net:\n",
    "            Y = blk(X)\n",
    "            \"\"\"\n",
    "            Concatenate the input and output of each block on the channel\n",
    "            dimension\n",
    "            \"\"\"\n",
    "            X = torch.cat((X, Y), dim=1)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* we define a convolution block with two blocks of 10 output channels\n",
    "* When using an input with 3 channels, we will get an output with the 3 + 2 × 10 = 23 channels. \n",
    "* The number of convolution block channels controls the increase in the number of output channels relative to the number of input channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 23, 8, 8])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blk = DenseBlock(2, 3, 10)\n",
    "X = torch.randn(4, 3, 8, 8)\n",
    "Y = blk(X)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ```Transition_Layers```\n",
    "* Since each dense block will increase the number of channels, adding too many of them will lead to an excessively complex model. \n",
    "    * A ```transition layer``` is used to control the complexity of the model.\n",
    "    * It reduces the number of channels by using the 1 × 1 convolutional layer and halves the height and width of the average pooling layer with a stride of 2, further reducing the complexity of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition_block(input_channels, num_channels):\n",
    "    layers = []\n",
    "    layers.append(nn.BatchNorm2d(input_channels))\n",
    "    layers.append(nn.ReLU())\n",
    "    layers.append(nn.Conv2d(input_channels, num_channels, kernel_size=1))\n",
    "    layers.append(nn.AvgPool2d(kernel_size=2, stride=2))\n",
    "    blk = nn.Sequential(*layers)\n",
    "    return blk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Apply a transition layer with 10 channels to the output of the dense block in the previous example. \n",
    "* This reduces the number of output channels to 10, and halves the height and width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 10, 4, 4])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blk = transition_block(23, 10)\n",
    "blk(Y).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "* DenseNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(x.shape[0], -1)\n",
    "\n",
    "class Reshape(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(-1,1,96,96)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ```DenseNet``` first uses the same single convolutional layer and maximum pooling layer as ```ResNet```.\n",
    "    * DenseNet uses four dense blocks. \n",
    "* we can set the number of convolutional layers used in each dense block. \n",
    "    * Here, we set it to 4,     \n",
    "* we set the number of channels (i.e. growth rate) for the convolutional layers in the dense block to 32, \n",
    "    * so 128 channels will be added to each dense block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = []\n",
    "layers.append(Reshape())\n",
    "layers.append(nn.Conv2d(in_channels=1, out_channels=64, kernel_size=7, stride=2, padding=3))\n",
    "layers.append(nn.BatchNorm2d(64))\n",
    "layers.append(nn.ReLU())\n",
    "layers.append(nn.MaxPool2d(kernel_size=3, stride=2, padding=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In ResNet, the height and width are reduced between each module by a residual block with a stride of 2. \n",
    "* Here, we use the transition layer to halve the height and width and halve the number of channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Num_channels: the current number of channels\n",
    "num_channels, growth_rate = 64, 32\n",
    "num_convs_in_dense_blocks = [4, 4, 4, 4]\n",
    "\n",
    "for i, num_convs in enumerate(num_convs_in_dense_blocks):\n",
    "    layers.append(DenseBlock(num_convs, num_channels, growth_rate))\n",
    "    # This is the number of output channels in the previous dense block\n",
    "    num_channels += num_convs * growth_rate\n",
    "    \"\"\"\n",
    "    A transition layer that haves the number of channels is added between\n",
    "    the dense blocks\"\"\"\n",
    "    \n",
    "    if i != len(num_convs_in_dense_blocks) - 1:\n",
    "        layers.append(transition_block(num_channels, num_channels // 2))\n",
    "        num_channels = num_channels // 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Similar to ResNet, a global pooling layer and fully connected layer are connected at the end to produce the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers.append(nn.BatchNorm2d(num_channels))\n",
    "layers.append(nn.ReLU())\n",
    "layers.append(nn.AdaptiveMaxPool2d((1,1)))\n",
    "layers.append(Flatten())\n",
    "layers.append(nn.Linear(num_channels, 10))\n",
    "\n",
    "net = nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "* Check the model design \n",
    "* To have a reasonable training time on Fashion-MNIST, we reduce the input height and width from 224 to 96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshape output shape:\t torch.Size([1, 1, 96, 96])\n",
      "Conv2d output shape:\t torch.Size([1, 64, 48, 48])\n",
      "BatchNorm2d output shape:\t torch.Size([1, 64, 48, 48])\n",
      "ReLU output shape:\t torch.Size([1, 64, 48, 48])\n",
      "MaxPool2d output shape:\t torch.Size([1, 64, 24, 24])\n",
      "DenseBlock output shape:\t torch.Size([1, 192, 24, 24])\n",
      "Sequential output shape:\t torch.Size([1, 96, 12, 12])\n",
      "DenseBlock output shape:\t torch.Size([1, 224, 12, 12])\n",
      "Sequential output shape:\t torch.Size([1, 112, 6, 6])\n",
      "DenseBlock output shape:\t torch.Size([1, 240, 6, 6])\n",
      "Sequential output shape:\t torch.Size([1, 120, 3, 3])\n",
      "DenseBlock output shape:\t torch.Size([1, 248, 3, 3])\n",
      "BatchNorm2d output shape:\t torch.Size([1, 248, 3, 3])\n",
      "ReLU output shape:\t torch.Size([1, 248, 3, 3])\n",
      "AdaptiveMaxPool2d output shape:\t torch.Size([1, 248, 1, 1])\n",
      "Flatten output shape:\t torch.Size([1, 248])\n",
      "Linear output shape:\t torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(size=(1, 1, 96, 96))   # [B, C, H, W]\n",
    "\n",
    "for layer in net:\n",
    "    X = layer(X)\n",
    "    print(layer.__class__.__name__,'output shape:\\t', X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "* Data Acquisition \n",
    "* Reading Data (Fashion-MNIST)\n",
    "* Preprocess: Fashion-MNIST has 28x28 pixels -> upsample them to 96x96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "import os \n",
    "\n",
    "import torchvision \n",
    "from torchvision import transforms \n",
    "from torch.utils.data import DataLoader \n",
    "\n",
    "def load_data_fashion_mnist(batch_size, resize=None, root=os.path.join(os.getcwd(), 'datasets', 'fashion-mnist')):\n",
    "    \"\"\"Download the Fashion-MNIST dataset and then load into memory.\"\"\"\n",
    "    root = os.path.expanduser(root)\n",
    "    transformer = []\n",
    "    if resize:\n",
    "        transformer += [transforms.Resize(resize)]\n",
    "    transformer += [transforms.ToTensor()]\n",
    "    transformer = transforms.Compose(transformer)\n",
    "\n",
    "    mnist_train = torchvision.datasets.FashionMNIST(root=root, train=True, transform=transformer, download=True)\n",
    "    mnist_test = torchvision.datasets.FashionMNIST(root=root, train=False, transform=transformer, download=True)\n",
    "    num_workers = 0 if sys.platform.startswith('win32') else 4\n",
    "\n",
    "    train_iter = DataLoader(mnist_train, batch_size, shuffle=True, num_workers=num_workers)\n",
    "    test_iter = DataLoader(mnist_test, batch_size, shuffle=False, num_workers=num_workers)\n",
    "    return train_iter, test_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "#Loading Fashion-MNIST Dataset\n",
    "train_iter, test_iter = load_data_fashion_mnist(batch_size, resize=96)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "* Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_gpu():\n",
    "    \"\"\"If GPU is available, return torch.device as cuda:0; else return torch.device as cpu.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda:0')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iter, net, device=torch.device('cpu')):\n",
    "    \"\"\"Evaluate accuracy of a model on the given data set.\"\"\"\n",
    "    net.eval()  # Switch to evaluation mode for Dropout, BatchNorm etc layers.\n",
    "    acc_sum, n = torch.tensor([0], dtype=torch.float32, device=device), 0\n",
    "    for X, y in data_iter:\n",
    "        # Copy the data to device.\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        with torch.no_grad():\n",
    "            y = y.long()\n",
    "            acc_sum += torch.sum((torch.argmax(net(X), dim=1) == y))\n",
    "            n += y.shape[0]\n",
    "    return acc_sum.item()/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ch5(net, train_iter, test_iter, criterion, num_epochs, batch_size, device, lr=None):\n",
    "    \"\"\"Train and evaluate a model with CPU or GPU.\"\"\"\n",
    "    print('training on', device)\n",
    "    net.to(device)\n",
    "    optimizer = optim.SGD(net.parameters(), lr=lr)\n",
    "    for epoch in range(num_epochs):\n",
    "        net.train() # Switch to training mode\n",
    "        n, start = 0, time.time()\n",
    "        train_l_sum = torch.tensor([0.0], dtype=torch.float32, device=device)\n",
    "        train_acc_sum = torch.tensor([0.0], dtype=torch.float32, device=device)\n",
    "        for X, y in train_iter:\n",
    "            optimizer.zero_grad()\n",
    "            X, y = X.to(device), y.to(device) \n",
    "            y_hat = net(X)\n",
    "            loss = criterion(y_hat, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            with torch.no_grad():\n",
    "                y = y.long()\n",
    "                train_l_sum += loss.float()\n",
    "                train_acc_sum += (torch.sum((torch.argmax(y_hat, dim=1) == y))).float()\n",
    "                n += y.shape[0]\n",
    "\n",
    "        test_acc = evaluate_accuracy(test_iter, net, device) \n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec'\\\n",
    "            % (epoch + 1, train_l_sum/n, train_acc_sum/n, test_acc, time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr, num_epochs, batch_size, device =  0.05, 5, 256, try_gpu()\n",
    "\n",
    "#Xavier initialization of weights\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.apply(init_weights)\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on cuda:0\n",
      "epoch 1, loss 0.0041, train acc 0.815, test acc 0.865, time 20.1 sec\n",
      "epoch 2, loss 0.0024, train acc 0.887, test acc 0.884, time 20.0 sec\n",
      "epoch 3, loss 0.0020, train acc 0.904, test acc 0.897, time 20.0 sec\n",
      "epoch 4, loss 0.0018, train acc 0.915, test acc 0.800, time 20.2 sec\n",
      "epoch 5, loss 0.0016, train acc 0.923, test acc 0.842, time 20.3 sec\n"
     ]
    }
   ],
   "source": [
    "#Loss Function Criterion\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_ch5(net, train_iter, test_iter, criterion, num_epochs, batch_size, device, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_env] *",
   "language": "python",
   "name": "conda-env-pytorch_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
