{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* a careless use of dense(= Linear) layers might give up the spatial structure of the representation entirely\n",
    "* Network in Network (NiN) blocks offer an alternative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ```NiN``` Blocks \n",
    "* The idea behind ```NiN``` is to apply a fully-connected layer at each pixel location (for each height and width).\n",
    "* If we tie the weights across each spatial location, we could think of this as a $1\\times 1$ convolutional layer, or as a fully-connected layer acting independently on each pixel location\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The ```NiN``` block consists of one convolutional layer followed by two $1\\times 1$ convolutional layers that act as per-pixel fully-connected layers with ReLU activations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* nxn Conv, #out_channels \n",
    "* ReLU\n",
    "* 1x1 Conv, #out_channels\n",
    "* ReLU\n",
    "* 1x1 Conv, #out_channels\n",
    "* ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nin_block(in_channels, out_channels, kernel_size, strides, padding):\n",
    "    blk = nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size, strides, padding),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(out_channels, out_channels, kernel_size=1), \n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(out_channels, out_channels, kernel_size=1), \n",
    "        nn.ReLU(),   \n",
    "        )\n",
    "    return blk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* NiN Model \n",
    "* ```NiN``` avoids dense connections altogether to avoid overfitting\n",
    "* Instead, ```NiN``` uses an ```NiN``` block with a number of output channels equal to the number of label classes, followed by a global average pooling layer, yielding a vector of logits.\n",
    "\n",
    "***\n",
    "\n",
    "* Advantage of NiN's design : it significantly reduces the number of required model parameters.\n",
    "* Disadvantage :  in practice, this design sometimes requires increased model training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* NiN_block, \n",
    "* 3x3 MaxPool, s=2 \n",
    "* NiN_block, \n",
    "* 3x3 MaxPool, s=2 \n",
    "* NiN_block, \n",
    "* 3x3 MaxPool, s=2 \n",
    "* Dropout, 50% \n",
    "* NiN_block, \n",
    "* AdaptiveMaxPool \n",
    "* Flatten "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.n1 = nin_block(1,out_channels=96, kernel_size=11, strides=4, padding=0)\n",
    "        self.m1 = nn.MaxPool2d(3,stride=2)\n",
    "        self.n2 = nin_block(96,out_channels=256, kernel_size=5, strides=1, padding=2)\n",
    "        self.m2 = nn.MaxPool2d(3,stride=2)\n",
    "        self.n3 = nin_block(256,out_channels=384, kernel_size=3, strides=1, padding=1)\n",
    "        self.m3 = nn.MaxPool2d(3,stride=2)\n",
    "        self.dropout1 = nn.Dropout2d(0.5)\n",
    "        self.n4 = nin_block(384,out_channels=10, kernel_size=3, strides=1, padding=1)\n",
    "        #Global Average Pooling can be achieved by AdaptiveMaxPool2d with output size = (1,1)\n",
    "        self.avg1 = nn.AdaptiveMaxPool2d((1,1))\n",
    "        self.flat = Flatten()\n",
    "        \n",
    "    def forward(self, x): \n",
    "        x = self.m1(self.n1(x))\n",
    "        x = self.m2(self.n2(x))\n",
    "        x = self.dropout1(self.m3(self.n3(x)))\n",
    "        x = self.n4(x)\n",
    "        x = self.avg1(x)\n",
    "        x = self.flat(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "* Create a data example to see the output shape of each block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential output shape:\t torch.Size([1, 96, 54, 54])\n",
      "MaxPool2d output shape:\t torch.Size([1, 96, 26, 26])\n",
      "Sequential output shape:\t torch.Size([1, 256, 26, 26])\n",
      "MaxPool2d output shape:\t torch.Size([1, 256, 12, 12])\n",
      "Sequential output shape:\t torch.Size([1, 384, 12, 12])\n",
      "MaxPool2d output shape:\t torch.Size([1, 384, 5, 5])\n",
      "Dropout2d output shape:\t torch.Size([1, 384, 5, 5])\n",
      "Sequential output shape:\t torch.Size([1, 10, 5, 5])\n",
      "AdaptiveMaxPool2d output shape:\t torch.Size([1, 10, 1, 1])\n",
      "Flatten output shape:\t torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(size=(1,1,224,224))  # [B, C, H, W]\n",
    "\n",
    "for layer in net.children():\n",
    "    X = layer(X)\n",
    "    print(layer.__class__.__name__,'output shape:\\t', X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "* Data Acquisition \n",
    "* Reading Data (Fashion-MNIST)\n",
    "* Preprocess: Fashion-MNIST has 28x28 pixels -> upsample them to 244x244"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "import os \n",
    "\n",
    "import torchvision \n",
    "from torchvision import transforms \n",
    "from torch.utils.data import DataLoader \n",
    "\n",
    "def load_data_fashion_mnist(batch_size, resize=None, root=os.path.join(os.getcwd(), 'datasets', 'fashion-mnist')):\n",
    "    \"\"\"Download the Fashion-MNIST dataset and then load into memory.\"\"\"\n",
    "    root = os.path.expanduser(root)\n",
    "    transformer = []\n",
    "    if resize:\n",
    "        transformer += [transforms.Resize(resize)]\n",
    "    transformer += [transforms.ToTensor()]\n",
    "    transformer = transforms.Compose(transformer)\n",
    "\n",
    "    mnist_train = torchvision.datasets.FashionMNIST(root=root, train=True, transform=transformer, download=True)\n",
    "    mnist_test = torchvision.datasets.FashionMNIST(root=root, train=False, transform=transformer, download=True)\n",
    "    num_workers = 0 if sys.platform.startswith('win32') else 4\n",
    "\n",
    "    train_iter = DataLoader(mnist_train, batch_size, shuffle=True, num_workers=num_workers)\n",
    "    test_iter = DataLoader(mnist_test, batch_size, shuffle=False, num_workers=num_workers)\n",
    "    return train_iter, test_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_iter, test_iter = load_data_fashion_mnist(batch_size, resize=224)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "* Model Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_gpu():\n",
    "    \"\"\"If GPU is available, return torch.device as cuda:0; else return torch.device as cpu.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda:0')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iter, net, device=torch.device('cpu')):\n",
    "    \"\"\"Evaluate accuracy of a model on the given data set.\"\"\"\n",
    "    net.eval()  # Switch to evaluation mode for Dropout, BatchNorm etc layers.\n",
    "    acc_sum, n = torch.tensor([0], dtype=torch.float32, device=device), 0\n",
    "    for X, y in data_iter:\n",
    "        # Copy the data to device.\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        with torch.no_grad():\n",
    "            y = y.long()\n",
    "            acc_sum += torch.sum((torch.argmax(net(X), dim=1) == y))\n",
    "            n += y.shape[0]\n",
    "    return acc_sum.item()/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ch5(net, train_iter, test_iter, criterion, num_epochs, batch_size, device, lr=None):\n",
    "    \"\"\"Train and evaluate a model with CPU or GPU.\"\"\"\n",
    "    print('training on', device)\n",
    "    net.to(device)\n",
    "    optimizer = optim.SGD(net.parameters(), lr=lr)\n",
    "    for epoch in range(num_epochs):\n",
    "        net.train() # Switch to training mode\n",
    "        n, start = 0, time.time()\n",
    "        train_l_sum = torch.tensor([0.0], dtype=torch.float32, device=device)\n",
    "        train_acc_sum = torch.tensor([0.0], dtype=torch.float32, device=device)\n",
    "        for X, y in train_iter:\n",
    "            optimizer.zero_grad()\n",
    "            X, y = X.to(device), y.to(device) \n",
    "            y_hat = net(X)\n",
    "            loss = criterion(y_hat, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            with torch.no_grad():\n",
    "                y = y.long()\n",
    "                train_l_sum += loss.float()\n",
    "                train_acc_sum += (torch.sum((torch.argmax(y_hat, dim=1) == y))).float()\n",
    "                n += y.shape[0]\n",
    "\n",
    "        test_acc = evaluate_accuracy(test_iter, net, device) \n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec'\\\n",
    "            % (epoch + 1, train_l_sum/n, train_acc_sum/n, test_acc, time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr, num_epochs, batch_size, device =  0.1, 5, 128, try_gpu()\n",
    "\n",
    "#Xavier initialization of weights\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.apply(init_weights)\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on cuda:0\n",
      "epoch 1, loss 0.0166, train acc 0.222, test acc 0.460, time 41.5 sec\n",
      "epoch 2, loss 0.0078, train acc 0.640, test acc 0.750, time 41.6 sec\n",
      "epoch 3, loss 0.0048, train acc 0.765, test acc 0.765, time 41.9 sec\n",
      "epoch 4, loss 0.0041, train acc 0.802, test acc 0.824, time 42.1 sec\n",
      "epoch 5, loss 0.0037, train acc 0.821, test acc 0.843, time 42.1 sec\n"
     ]
    }
   ],
   "source": [
    "train_iter, test_iter = load_data_fashion_mnist(batch_size, resize=224)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_ch5(net, train_iter, test_iter, criterion, num_epochs, batch_size, device, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_env] *",
   "language": "python",
   "name": "conda-env-pytorch_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
