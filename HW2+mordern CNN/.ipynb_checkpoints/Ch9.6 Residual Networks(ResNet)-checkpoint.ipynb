{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Even more important is the ability to design networks where adding layers makes networks strictly more expressive rather than just different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ```Residual_Blocks```\n",
    "* ResNet follows VGG’s full 3 × 3 convolutional layer design. The residual block has two 3 × 3 convolutional layers with the same number of output channels.\n",
    "* Each convolutional layer is followed by a batch normal- ization layer and a ReLU activation function. Then, we skip these two convolution operations and add the input directly before the final ReLU activation function.\n",
    "* This kind of design requires that the output of the two convolutional layers be of the same shape as the input, so that they can be added together.\n",
    "* If we want to change the number of channels or the the stride, we need to introduce an additional 1 × 1 convolutional layer to transform the input into the desired shape for the addition operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This code generates two types of networks:\n",
    "    * one where we add the input to the output before applying the ReLU nonlinearity\n",
    "    * whenever ```use_1x1conv=True```, one where we adjust channels and resolution by means of a 1 × 1 convolution before adding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(nn.Module):\n",
    "  \n",
    "  def __init__(self,input_channels, num_channels, use_1x1conv=False, strides=1, **kwargs):\n",
    "    super(Residual, self).__init__(**kwargs)\n",
    "    self.conv1 = nn.Conv2d(input_channels, num_channels,kernel_size=3, padding=1, stride=strides)\n",
    "    self.conv2 = nn.Conv2d(num_channels, num_channels, kernel_size=3, padding=1)\n",
    "    if use_1x1conv:\n",
    "        self.conv3 = nn.Conv2d(input_channels, num_channels, kernel_size=1, stride=strides)\n",
    "    else:\n",
    "        self.conv3 = None\n",
    "    self.bn1 = nn.BatchNorm2d(num_channels)\n",
    "    self.bn2 = nn.BatchNorm2d(num_channels)\n",
    "    self.relu = nn.ReLU(inplace=True)\n",
    "  \n",
    "  def forward(self, X):\n",
    "    \n",
    "    Y = self.relu(self.bn1(self.conv1(X)))\n",
    "    Y = self.bn2(self.conv2(Y))\n",
    "    if self.conv3:\n",
    "        X = self.conv3(X)\n",
    "    Y += X\n",
    "    Y =self.relu(Y)\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* let's look at a situation where the input and output are of the same shape.\n",
    "    * Input  :(4,3,6,6) shape \n",
    "    * Output :(4,3,6,6) shape  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 6, 6])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blk = Residual(3,3)\n",
    "X = torch.rand(4, 3, 6, 6)\n",
    "\n",
    "Y = blk(X)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We also have the option to halve the output height and width while increasing the number of output channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 6, 3, 3])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blk = Residual(3,6, use_1x1conv=True, strides=2)\n",
    "blk(X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ```ResNet Model``` \n",
    "* The first two layers of ResNet are the same as those of the GoogLeNet we described before: \n",
    "    * 7x7 Conv,s=2, p=3, #64 \n",
    "    * BN \n",
    "    * ReLU\n",
    "    * 3x3 MaxPool, s=2, p=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1 = nn.Sequential(nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),\n",
    "                    nn.BatchNorm2d(64),\n",
    "                    nn.ReLU(),\n",
    "                    nn.MaxPool2d(kernel_size=3, stride=2, padding=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ResNet uses four modules made up of residual blocks, each of which uses several residual blocks with the same number of output channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_block(input_channels, num_channels, num_residuals, first_block=False):\n",
    "    blk = []\n",
    "    for i in range(num_residuals):\n",
    "        if i == 0 and not first_block:\n",
    "            blk.append(Residual(input_channels, num_channels, use_1x1conv=True, strides=2))\n",
    "        else:\n",
    "            blk.append(Residual(num_channels, num_channels))\n",
    "    return blk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Then, we add all the residual blocks to ResNet. Here, two residual blocks are used for each module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There are 4 convolutional layers in each module (excluding the 1 × 1 convolutional layer).\n",
    "* Together with the first convolutional layer and the final fully connected layer, there are 18 layers in total.\n",
    "    * this model is commonly known as ```ResNet-18```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "b2=nn.Sequential(*resnet_block(64,64,2,first_block=True))\n",
    "b3=nn.Sequential(*resnet_block(64,128,2))\n",
    "b4=nn.Sequential(*resnet_block(128,256,2))\n",
    "b5=nn.Sequential(*resnet_block(256,512,2))\n",
    "\n",
    "net=nn.Sequential(b1,b2,b3,b4,b5,nn.AdaptiveMaxPool2d((1,1)),Flatten(),nn.Linear(512, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* By configuring different numbers of channels and residual blocks in the module, we can create different ResNet models, such as the deeper 152-layer ```ResNet-152```.\n",
    "* Although the main architecture of ResNet is similar to that of GoogLeNet, ResNet’s structure is simpler and easier to modify.\n",
    "    * All these factors have resulted in the rapid and widespread use of ResNet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "* Check the model design \n",
    "* To have a reasonable training time on Fashion-MNIST, we reduce the input height and width from 224 to 96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential output shape:\t torch.Size([1, 64, 24, 24])\n",
      "Sequential output shape:\t torch.Size([1, 64, 24, 24])\n",
      "Sequential output shape:\t torch.Size([1, 128, 12, 12])\n",
      "Sequential output shape:\t torch.Size([1, 256, 6, 6])\n",
      "Sequential output shape:\t torch.Size([1, 512, 3, 3])\n",
      "AdaptiveMaxPool2d output shape:\t torch.Size([1, 512, 1, 1])\n",
      "Flatten output shape:\t torch.Size([1, 512])\n",
      "Linear output shape:\t torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(size=(1, 1, 96, 96))   # [B, C, H, W]\n",
    "\n",
    "for layer in net:\n",
    "    X = layer(X)\n",
    "    print(layer.__class__.__name__,'output shape:\\t', X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "* Data Acquisition \n",
    "* Reading Data (Fashion-MNIST)\n",
    "* Preprocess: Fashion-MNIST has 28x28 pixels -> upsample them to 96x96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "import os \n",
    "\n",
    "import torchvision \n",
    "from torchvision import transforms \n",
    "from torch.utils.data import DataLoader \n",
    "\n",
    "def load_data_fashion_mnist(batch_size, resize=None, root=os.path.join(os.getcwd(), 'datasets', 'fashion-mnist')):\n",
    "    \"\"\"Download the Fashion-MNIST dataset and then load into memory.\"\"\"\n",
    "    root = os.path.expanduser(root)\n",
    "    transformer = []\n",
    "    if resize:\n",
    "        transformer += [transforms.Resize(resize)]\n",
    "    transformer += [transforms.ToTensor()]\n",
    "    transformer = transforms.Compose(transformer)\n",
    "\n",
    "    mnist_train = torchvision.datasets.FashionMNIST(root=root, train=True, transform=transformer, download=True)\n",
    "    mnist_test = torchvision.datasets.FashionMNIST(root=root, train=False, transform=transformer, download=True)\n",
    "    num_workers = 0 if sys.platform.startswith('win32') else 4\n",
    "\n",
    "    train_iter = DataLoader(mnist_train, batch_size, shuffle=True, num_workers=num_workers)\n",
    "    test_iter = DataLoader(mnist_test, batch_size, shuffle=False, num_workers=num_workers)\n",
    "    return train_iter, test_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "#Loading Fashion-MNIST Dataset\n",
    "train_iter, test_iter = load_data_fashion_mnist(batch_size, resize=96)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "* Model Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_gpu():\n",
    "    \"\"\"If GPU is available, return torch.device as cuda:0; else return torch.device as cpu.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda:0')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iter, net, device=torch.device('cpu')):\n",
    "    \"\"\"Evaluate accuracy of a model on the given data set.\"\"\"\n",
    "    net.eval()  # Switch to evaluation mode for Dropout, BatchNorm etc layers.\n",
    "    acc_sum, n = torch.tensor([0], dtype=torch.float32, device=device), 0\n",
    "    for X, y in data_iter:\n",
    "        # Copy the data to device.\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        with torch.no_grad():\n",
    "            y = y.long()\n",
    "            acc_sum += torch.sum((torch.argmax(net(X), dim=1) == y))\n",
    "            n += y.shape[0]\n",
    "    return acc_sum.item()/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ch5(net, train_iter, test_iter, criterion, num_epochs, batch_size, device, lr=None):\n",
    "    \"\"\"Train and evaluate a model with CPU or GPU.\"\"\"\n",
    "    print('training on', device)\n",
    "    net.to(device)\n",
    "    optimizer = optim.SGD(net.parameters(), lr=lr)\n",
    "    for epoch in range(num_epochs):\n",
    "        net.train() # Switch to training mode\n",
    "        n, start = 0, time.time()\n",
    "        train_l_sum = torch.tensor([0.0], dtype=torch.float32, device=device)\n",
    "        train_acc_sum = torch.tensor([0.0], dtype=torch.float32, device=device)\n",
    "        for X, y in train_iter:\n",
    "            optimizer.zero_grad()\n",
    "            X, y = X.to(device), y.to(device) \n",
    "            y_hat = net(X)\n",
    "            loss = criterion(y_hat, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            with torch.no_grad():\n",
    "                y = y.long()\n",
    "                train_l_sum += loss.float()\n",
    "                train_acc_sum += (torch.sum((torch.argmax(y_hat, dim=1) == y))).float()\n",
    "                n += y.shape[0]\n",
    "\n",
    "        test_acc = evaluate_accuracy(test_iter, net, device) \n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec'\\\n",
    "            % (epoch + 1, train_l_sum/n, train_acc_sum/n, test_acc, time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr, num_epochs, batch_size, device =  0.05, 5, 256, try_gpu()\n",
    "\n",
    "#Xavier initialization of weights\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.apply(init_weights)\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on cuda:0\n",
      "epoch 1, loss 0.0084, train acc 0.765, test acc 0.862, time 25.2 sec\n",
      "epoch 2, loss 0.0025, train acc 0.880, test acc 0.888, time 25.8 sec\n",
      "epoch 3, loss 0.0020, train acc 0.907, test acc 0.892, time 26.0 sec\n",
      "epoch 4, loss 0.0019, train acc 0.911, test acc 0.873, time 26.0 sec\n",
      "epoch 5, loss 0.0015, train acc 0.930, test acc 0.910, time 25.6 sec\n"
     ]
    }
   ],
   "source": [
    "#Loss Function Criterion\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_ch5(net, train_iter, test_iter, criterion, num_epochs, batch_size, device, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_env] *",
   "language": "python",
   "name": "conda-env-pytorch_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
